{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality Assesment Form\n",
    "\n",
    "This Jupyter Notebook provides the source used to create the tables from the Quality Assesment Form responses.  \n",
    "The following steps are performed:\n",
    "1. Load responses\n",
    "2. Separate answers and scores\n",
    "3. Export the tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Loading the dataset\n",
    "\n",
    "Google Forms was used to collect the responses for the Data Extraction Form.  \n",
    "The responses were exported first to Google Sheets and then from Google Sheets to a CSV file.  \n",
    "Finally, the CSV file is loaded using the pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "qaf_df = QualityAssessment().data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Creating and validate the scores dataframe\n",
    "\n",
    "The scores dataframe can be directly obtained from the Quality Assement Form responses.  \n",
    "Each paper has a score ranging from 0.0 to 10.0, according to the answer to each question.  \n",
    "Each __Yes__ answer yields 1.0 point, while __Partially__ and __No__ answers yield 0.5, and 0 points, respectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True    82\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_df = (qaf_df.replace(to_replace=QualityAssessment.YES, value=1.0)\n",
    "               .replace(to_replace=QualityAssessment.PARTIALLY, value=0.5)\n",
    "               .replace(to_replace=QualityAssessment.NO, value=0.0))\n",
    "\n",
    "\n",
    "(answers_df.iloc[:, -11:-1].sum(axis=1) == qaf_df[QualityAssessment.SCORE]).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Creating the answers dataframe\n",
    "\n",
    "The answers dataframe is obtained from summarizing the original dataframe.  \n",
    "Each row of the original dataframe represents a paper, and each column represents a question.  \n",
    "For each question (column) there is the need to obtain the count of each answer.  \n",
    "These values are added in an auxiliary dataframe which contains three rows (one representing each possible answer).  \n",
    "The auxiliary dataframe is used to build the answers dataframe by appending to it at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\begin{table}[htb!]\n",
      "\t\\caption{Answers gathered in the QAF}\n",
      "\t\\begin{tabular}{@{}lrrrr@{}}\n",
      "\t\t\\toprule\n",
      "\t\tQuestion &   Yes & Partially &    No & Some level of \\\\\n",
      "\t\t\\midrule\n",
      "\t\tQA1      & 97.56 &      1.22 &  1.22 &         98.78 \\\\\n",
      "\t\tQA2      & 68.29 &     20.73 & 10.98 &         89.02 \\\\\n",
      "\t\tQA3      & 68.29 &     10.98 & 20.73 &         79.27 \\\\\n",
      "\t\tQA4      & 63.41 &      1.22 & 35.37 &         64.63 \\\\\n",
      "\t\tQA5      & 91.46 &      2.44 &  6.10 &         93.90 \\\\\n",
      "\t\tQA6      & 93.90 &      3.66 &  2.44 &         97.56 \\\\\n",
      "\t\tQA7      & 59.76 &     15.85 & 24.39 &         75.61 \\\\\n",
      "\t\tQA8      & 51.22 &     37.80 & 10.98 &         89.02 \\\\\n",
      "\t\tQA9      & 31.71 &     56.10 & 12.20 &         87.80 \\\\\n",
      "\t\tQA10     & 78.05 &     10.98 & 10.98 &         89.02 \\\\\n",
      "\t\t\\bottomrule\n",
      "\t\\end{tabular}\n",
      "\t\\label{tab:answers}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del qaf_df[QualityAssessment.SCORE]\n",
    "except KeyError:\n",
    "    pass\n",
    "    \n",
    "sanitized_answers_df = pd.DataFrame(columns=[QualityAssessment.YES, QualityAssessment.PARTIALLY, QualityAssessment.NO])\n",
    "for col in qaf_df:\n",
    "    count = qaf_df[col].value_counts()\n",
    "      \n",
    "    aux = pd.DataFrame()\n",
    "    aux[QualityAssessment.QUESTION] = pd.Series(col for i in range(len(count)))\n",
    "    aux[QualityAssessment.ANSWER] = count.index\n",
    "    aux[QualityAssessment.COUNT] = count.values\n",
    "    \n",
    "    aux.drop(QualityAssessment.QUESTION, axis=1, inplace=True)\n",
    "    aux = aux.T\n",
    "    aux = aux.rename(columns=aux.iloc[0]).drop(QualityAssessment.ANSWER).rename(index={QualityAssessment.COUNT: col})\n",
    "    aux = aux.reindex(sorted(aux.columns), axis=1)\n",
    "    \n",
    "    sanitized_answers_df = sanitized_answers_df.append(aux)\n",
    "    \n",
    "number_of_papers = len(qaf_df)\n",
    "sanitized_answers_df[QualityAssessment.SOME] = sanitized_answers_df[QualityAssessment.PARTIALLY] + sanitized_answers_df[QualityAssessment.YES]\n",
    "sanitized_answers_df = sanitized_answers_df.div(number_of_papers).mul(100).astype(float).round(2)\n",
    "sanitized_answers_df = sanitized_answers_df.rename_axis().reset_index().rename(columns={Table.INDEX: QualityAssessment.QUESTION})\n",
    "\n",
    "with open(f'{OUTPUT_PATH}/QAF-Answers.tex', 'w') as file:\n",
    "\n",
    "    table_header = (\n",
    "'''\n",
    "\\\\begin{table}[htb!]\n",
    "\\t\\\\caption{Answers gathered in the QAF}\n",
    "\\t\\\\begin{tabular}{@{}lrrrr@{}}\n",
    "\\t\\t\\\\toprule\n",
    "''')\n",
    "    \n",
    "    table_footer = (\n",
    "'''\n",
    "\\t\\t\\\\bottomrule\n",
    "\\t\\\\end{tabular}\n",
    "\\t\\\\label{tab:answers}\n",
    "\\\\end{table}\n",
    "''')\n",
    "    \n",
    "    file.write(table_header)\n",
    "    print(table_header, end='')\n",
    "    \n",
    "    formatted_header = f'\\t\\t{QualityAssessment.QUESTION:<4} & {QualityAssessment.YES:>5} & {QualityAssessment.PARTIALLY:>5} & {QualityAssessment.NO:>5} & {QualityAssessment.SOME:>4} \\\\\\\\'\n",
    "    \n",
    "    file.write(formatted_header)\n",
    "    print(formatted_header, end='')\n",
    "    \n",
    "    midrule = f'\\n\\t\\t\\\\midrule'\n",
    "    \n",
    "    file.write(midrule)\n",
    "    print(midrule, end='')\n",
    "    \n",
    "    for index, row in sanitized_answers_df.iterrows():\n",
    "        formatted_row = f'\\n\\t\\t{row[QualityAssessment.QUESTION].upper():<8} & {row[QualityAssessment.YES]:>4.2f} & {row[QualityAssessment.PARTIALLY]:>9.2f} & {row[QualityAssessment.NO]:>5.2f} & {row[QualityAssessment.SOME]:>13.2f} \\\\\\\\'\n",
    "        \n",
    "        file.write(formatted_row)\n",
    "        print(formatted_row, end='')\n",
    "    \n",
    "    file.write(table_footer)\n",
    "    print(table_footer)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Creating the scores table\n",
    "Here, we get the scores of papers and create the table used in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\begin{table}[htb!]\n",
      "\t\\caption{Scores of papers included in the primary selection}\n",
      "\t\\begin{tabular}{@{}rrrrr@{}}\n",
      "\t\t\\toprule\n",
      "\t\tScore & Amount & Cumulative Amount &    \\% & Cumulative \\% \\\\\n",
      "\t\t\\midrule\n",
      "\t\t  5.0 &      1 &                 1 &  1.22 &          1.22 \\\\\n",
      "\t\t  5.5 &      6 &                 7 &  7.32 &          8.54 \\\\\n",
      "\t\t  6.0 &      9 &                16 & 10.98 &         19.51 \\\\\n",
      "\t\t  6.5 &      5 &                21 &  6.10 &         25.61 \\\\\n",
      "\t\t  7.0 &      8 &                29 &  9.76 &         35.37 \\\\\n",
      "\t\t  7.5 &      6 &                35 &  7.32 &         42.68 \\\\\n",
      "\t\t  8.0 &     13 &                48 & 15.85 &         58.54 \\\\\n",
      "\t\t  8.5 &      5 &                53 &  6.10 &         64.63 \\\\\n",
      "\t\t  9.0 &     11 &                64 & 13.41 &         78.05 \\\\\n",
      "\t\t  9.5 &     16 &                80 & 19.51 &         97.56 \\\\\n",
      "\t\t 10.0 &      2 &                82 &  2.44 &        100.00 \\\\\n",
      "\t\t\\bottomrule\n",
      "\t\\end{tabular}\n",
      "\t\\label{tab:scores}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "scores_df = answers_df[QualityAssessment.SCORE]\n",
    "total_papers = len(scores_df)\n",
    "cumulative_sum = 0\n",
    "cumulative_percentage = 0\n",
    "\n",
    "with open(f'{OUTPUT_PATH}/QAF-Scores.tex', 'w') as file:\n",
    "    \n",
    "    percentage = '\\\\%'\n",
    "    \n",
    "    table_header = (\n",
    "'''\n",
    "\\\\begin{table}[htb!]\n",
    "\\t\\\\caption{Scores of papers included in the primary selection}\n",
    "\\t\\\\begin{tabular}{@{}rrrrr@{}}\n",
    "\\t\\t\\\\toprule\n",
    "''')\n",
    "    \n",
    "    table_footer = (\n",
    "'''\n",
    "\\t\\t\\\\bottomrule\n",
    "\\t\\\\end{tabular}\n",
    "\\t\\\\label{tab:scores}\n",
    "\\\\end{table}\n",
    "''')\n",
    "   \n",
    "    file.write(table_header)\n",
    "    print(table_header, end='')\n",
    "    \n",
    "    formatted_header = f'\\t\\t{\"Score\":>4} & {\"Amount\":>6} & {\"Cumulative Amount\":>17} & {percentage:>5} & {\"Cumulative\"} {percentage} \\\\\\\\'\n",
    "\n",
    "    file.write(formatted_header)\n",
    "    print(formatted_header)\n",
    "    \n",
    "    midrule = '\\t\\t\\\\midrule'\n",
    "    file.write(midrule)\n",
    "    print(midrule, end='')\n",
    "    \n",
    "    for score, amount in sorted(scores_df.value_counts().iteritems(), key=lambda x: x[0]):\n",
    "        percentage = amount / total_papers * 100\n",
    "        cumulative_sum += amount\n",
    "        cumulative_percentage += percentage\n",
    "        \n",
    "        formatted_row = f'\\n\\t\\t{score:>5} & {amount:>6} & {cumulative_sum:>17} & {percentage:>5.2f} & {cumulative_percentage:>13.2f} \\\\\\\\'\n",
    "        \n",
    "        file.write(formatted_row)\n",
    "        print(formatted_row, end='')\n",
    "        \n",
    "    file.write(table_footer)\n",
    "    print(table_footer, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Counting number of papers included and excluded\n",
    "Shows the amount of papers included (False) and excluded (True) with the Quality Assessment Form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    66\n",
       "True     16\n",
       "Name: score, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(scores_df < 6.5).value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
